All right.
Welcome to Happy Path Programming.
Let's see, before we get into Trisha Gere, wonderful guest for today.
We've got Winter Tech Forum coming up in a couple of weeks.
Two weeks, exactly, almost.
We're here in Crest, Dubuque, where we have lots of snow.
And you can still sign up, and you might be able to go to the Yurt Dinner.
It's hard to say.
But we've officially released the rest of the places.
But, you know, they can make a last minute call.
It's a good time.
It's going to be fun.
It's a really good time.
Yeah.
And also, just wanted to mention, I have a couple of books on Lean Pub.
There's Atomic Kotlin, and there's on Java 8.
And there's, I mean, increasingly, perhaps poorly chosen title, but people still get a lot out of it.
there the on java a on the on java 81 yeah nice cool right and then uh and trisha welcome and you have a book yes i see um i see your atomic kotlin book on lean pub a lot because when i'm checking the top 10 yours is in the top 10 the minds of number 37 that's really gratifying yes i have a book getting to know intelligent idea which is also on lean pub it's on amazon too but you know lean pub is uh
friendlier.
And you have a hardcover version.
So I did it on Lean Pub because it's a nice platform, self-published, and then when I released it, so I released the first edition, me and Helen actually, released the first edition November before last, and then I felt a bit down after it was done.
I was like, why do I feel down?
Because I can't hold it in my hands.
So then I looked into the KDP staff and so the Kindle Direct publishing, so you can
upload a PDF to Amazon and they'll publish you a paperback in the hardback you want to.
Oh, nice.
Yeah.
Yeah.
I feel down after a book because I realize the nice clear meaning of my life just suddenly stops, you know, but before it's like, okay, we are existing to create the book and then it goes away.
But it is nice to have the physical thing on the shelf behind you, which we're looking at Trisha's book behind her on the shelf.
And Bruce has his books behind him on the shelf.
I actually paid full price for Head First Java because I helped contribute to Head First Java.
And I think my author copies weren't due for ages, so I paid full price so I could physically hop the book.
Yeah, right.
No, it's good.
There's definitely something satisfying about that.
So real quick on that book, it goes into, I'm guessing, IntelliJ and how to use it more effectively and what else?
Because I was doing advocacy for IntelliJ at JetBrains for like seven years.
So at the end of that night, I left a couple of years ago, I'm like, what I really want to sort of download everything from my brain into something that other people can use.
So the book is split into four sections.
The first section is kind of
bit fluffy in terms of like how IntelliJ thinks, like where you need to be looking inside the IDE to look for the stuff that you care about.
The second two sections are all sort of tutorial driven.
And I use TDD with it as well because I wanted to show this is
this is how to get the most out of IntelliJ in terms of like it will do code generation and it will make everything compile.
And so there's two tutorial sections in the middle.
And then the fourth section is actually like half the book, which is like everything else you need to know about IntelliJ.
So it goes into depth about like run configurations and about like tips and tricks for debugging and how best to use the testing functionality.
And I keep thinking about splitting that fourth section out and just having it as a standalone book because it's just got so much useful stuff in there.
Nice.
Yes.
And so you wrote this book even after you left Jet Brands.
We started it, I was trying to remember, because like history or blurs into one, post pandemic, everything is big.
So me and Helen started writing it in my, what was to become my last year at JetBrains.
And then I took a sabbatical and like you were saying about the purpose of my life in the sabbatical, I had two purposes.
One was to help finish head first Java, which actually came, that came to me
after I started writing the IntelliJ book, so I knew I couldn't have a full-time job and write two books at the same time.
So I had like three months of Finnish head first Java and in some ways, I mean, that was a crazy time, but in some ways I sort of miss it because I'd be like, right, get up, get the kids to school, sit down and write the book and keep writing the game.
And then when that was done, then I went back in and spent another like six months on the IntelliJ IDEA book.
And I was like, we've got to get this out the door because we spent two years on it now.
And it was supposed to be an easy thing that we can just quickly.
I need a sabbatical so we can finish the effect on your programming book.
Yeah, the siren song of the easy book.
Yeah, never turns out that way.
I think I might pick a hard book.
I was having a conversation with Brian Getz at DevOps about how do you feel about updating Java concurrency, was it modern Java concurrency?
That's a hard book.
I don't think that's something you can just get out the door.
Well, plus all the work that they've done on Lume has changed everything.
So I think you'd just want to write a new book, which would be really hard.
And what did he say?
No way on earth.
No, I mean, he was, he always said, I was asking him because he always said, when loom comes out, I'll consider it.
And so then he was sort of saying, I don't know if this is like, you know, conference dinner, non-disclosure, but he was sort of saying the same thing that you just said, you can't really update the book and just like plonk loom on the end of it.
Yeah.
So he was considering
having a part two book.
So not a second edition, but like modern, modern Java.
Exactly.
Like the first book is still valid.
All of that, none of that goes away.
It's just that that's not the first thing you're going to think about anymore.
Like how do we use fork join and
You're going to use the executors pool and and loom and and things like concurrent data structures and all the stuff that's kind of come in since I think that I think his book was what Java 5, Java 6.
So there's there's a lot to cover.
Yeah.
Yeah.
Not just a refactor, I would say.
I mean, if it was me, I would go, new book, you could draw from the material in there, but just make it a new book.
That's what I would do if you're listening, Brian.
Just my opinion.
Well, maybe eventually you'll have a fleet version of your IntelliJ book, and it'd be interesting to see, can you just do a refactor, or is that a full rewrite?
Yeah, I mean, fleet and some of the other options that I gotta say, I'm an old fashioned programmer.
I'm like, I spent a long time learning IntelliJ.
Like, I'm going to keep using IntelliJ.
Oh, yeah, once you invest a lot of time in something like that.
Yeah.
Know your tools.
It's hard to change them.
It is, it is hard and Helen is still working at JetBrains and she's more of a sort of broad knowledge person so she's dipping in and out of the IDE so sort of hoping that maybe she might tell me what's different in the IDE's but I'm not going to spend 15 years learning fleet so that I can do a fleet book as well.
Yeah, well, and I've tried it a couple of times and my initial reaction has always been, oh, well, this isn't, I mean,
This isn't as responsive as VS code yet.
So, I'm not, which I think is what they're trying to compete with, right?
Pretty sure.
The thought that I have when I use fleet is, oh, this isn't for me.
Not because it's bad, but it's just, I'm not the sort of programmer they're targeting.
And I think that's how I feel when I use VS code.
It's like, yeah, it's similar.
I'm, yeah, I don't know, I'm still, I mean, I use both VS Code and IntelliJ, but more, I don't know, most of the time, it's like, it's the little things.
I find it's those little hurdles.
And this is, you know, developer productivity is one of your things.
So we can, we can work into that.
But
Those little hurdles are what get me and starting up IntelliJ and getting into that mindset versus just bringing up VS Code.
Most of the time I'll just use VS Code and then I learn it.
I learned more and more about it in the process.
but it's that.
It's almost just the startup time that gets me.
And also the simplicity.
It hasn't got this huge surface area.
And I've discovered this in a bunch of things.
It's the little hurdles that get you.
And maybe we can move into the idea of test-driven development and testing in general, because that's a big thing for you.
Because I just
Recently in the last few days came across an article in my newsfeed where the guy was talking about, I don't remember, it was basically the things that didn't work in test-driven development.
Because a lot of times, TDD is something that people will go, oh, well, you have to use TDD everywhere.
And he's pointing out that, well, a lot of times, if you're doing exploratory programming, you don't know.
If you're doing, if you're just trying to solve a specific problem, the test is really, is the problem solved or not?
And so you don't necessarily build a whole bunch of tests and you certainly don't do the test first thing.
And it's like, there's this kind of narrow area where you go, oh, we have the complete specification.
We can start by writing the tests for that specification that fail and then, you know, but that's like not that big of a,
It's not holistic of all the different things that you're doing in the activity of programming.
Yeah, because so much of the time you don't have that full specification.
Yeah.
And if you do, sure, I can see why you'd use TD.
Wouldn't the counter argument to that be that, well, you should probably go build that specification before you start writing code?
Um, I, I guess, but it, but, but if it's, you know, if I'm just, well, certainly if I'm exploring, I don't know what that specification would look like.
I'm just trying to figure out, is it possible to do this thing or do I want to do this thing or whatever?
So what do you think about this?
I mean, everything's about gray areas, isn't it?
Right.
And I'm a big fan of TDD.
Um, I tend to write my tests first, even when I, so,
When I used to interview people for a position where we had to have TDD as part of the job, people would say, yeah, I write my test first, except when I don't know what I'm doing.
And then I write the code first.
And I'm a bit like, yeah, but if you don't know what you're doing, why are you writing the production code?
Part of the thing about the test is to write down the things that you don't know.
Even if it's just the title of the test, what should it do if the connection fails?
What should it do if the user gives you some garbage input?
What should it do if, I don't know, if you don't know what you're doing?
So I quite often use the test to make a note of all those things that I don't know, and then kind of use that to drive the solution.
But it does depend a little bit on the application you're working on and how it's architectured, architected, and also what sorts of tests you're writing.
So when I worked at Elmax, we wrote a whole bunch of automated acceptance tests.
So you can be quite high level in terms of
the behavior the user expects and we wrote the unit test too when you're doing the unit test you're going to be more precise about like it is definitely going to do these things when you do these sorts of things and on the acceptance test level you can write these automated tests which are like well you know it should definitely give me some sort of error if I do something stupid.
But if you don't have those different levels of testing in a unit test where your unit tests are like one test, one class is very difficult to write that general.
Well, I want it to do something definitely wrong when this weird stuff happens.
Yeah.
Yeah.
Yeah.
So there's like a spectrum with testing where I
don't often write a lot of unit tests because a lot of the stuff that I'm building is really about integration and it's the integration points that are brittle and can fail or be flaky and so those are the places where I often start is like like what how should the integration work and so it's maybe a little
unfortunate in some cases that I can't break that down into a unit test that has much value.
But really what I care about is like, is this integration working as I expect it to?
But then that makes my tests slower, other integration tests and the system that I'm relying on maybe needs a bunch of setup in order to have a valid test.
My husband writes a lot of Android stuff.
He and I both worked at the same place in London where we worked with Dave Farley with these acceptance tests and stuff.
He wants to test drive a lot of stuff the same way that we did there.
But he does find it, it's a very different beast when you're working with Android because you can't just fire up a quick unit test that does the thing that you want.
But what he has found is that he often writes these integration tests or emulator level tests because
You can think that way in terms of the overall flow.
But then afterwards, he will sometimes refact those down to unit tests and be like, oh, you know what?
I don't need the emulator for this.
I'm actually only testing this part of the view or whatever.
But again, I guess that's one way that I have to do it is I'll start with the integration test.
And then I realize in trying to fulfill the integration test that, oh, there's actually a few places that I can break down into being unit tests.
But I guess I'm almost in my TDD
And a lot of stuff I build, I'm an integration test first kind of approach and then identifying the places that could be unit test.
Yeah, because it seems like that's not driven.
I guess that's the problem that I have when people are very rigid about, oh, you always have to write the test before you write any code.
And I have discovered that
When you're you know at some point well it's almost like when you're writing a book you write an example and then you start writing the pros and it changes or at least in my experience it often changes the example you go oh wait you know you're sort of rubber ducking.
the example down on the thing.
And so for me, testing will often do that.
And you go like, even just to make something testable, you go, Oh yeah, that would change the design of it.
But that isn't upfront.
And I think maybe my problem isn't, it's the, the driven part, the, Oh, you have to get this test first.
Yeah.
And then fill in the, you know, connect the dots afterwards.
I think for me, the key thing is to be able to switch between the two, like rapidly, because you can't just write 300 tests with no implementation and then write the implementation and expect everything to pass.
Like whether you write the implementation and go, oh, now I know what the test is supposed to look like, or whether it's about going backwards and forwards.
Because the other thing is that one of the, I went from this place I was working with Dave Farley, I started working at MongoDB.
So I was trying to do test-driven stuff there.
The pattern that they had been using is that, because we were working on the Java driver, so we had a specification on how the Java driver was going to talk to the database.
So a lot of the design was sort of driven from the database, like upwards, if you like, which kind of makes sense, because we have a specification on how to talk to the database, and so you're sort of exposing that to the end user, and the end user is a Java developer who's trying to talk to MongoDB.
But when you start thinking test first, you start thinking from the outside in,
you start to realize that the API makes no sense when you're exposing what the database cares about.
And when I'm a developer, I don't really care about whatever it is the database cares about.
I want an insert statement, which is going to take the thing I'm inserting and maybe some criteria.
And in Java syntax, I would expect it to look like this.
And so I sometimes find test driven development.
That's why I learned that test driven development is very good for API design, because then you're starting to think,
From the outside, what makes sense?
What shape should it be?
Instead of doing what we often do as developers and go, here's what you've got.
Just do your best with this.
Yeah, this is what makes sense based on the system that I'm abstracting over when the other way is to think, okay, this is what the developer actually wants from their API.
Right, and I think the biggest thing that I've come away with is when you
do the testing and you go, oh, wait, this doesn't make sense.
I need to change the API to make it testable.
It often or almost always improves the API in that process.
So that hand in hand approach is just really essential, I think.
Yeah, I think as long as you're doing both at the same time and evolving in both directions, because if you if you write your production code and then don't write any test, well, A, that's bad.
And B, when you come to it six months later and go, Oh, it doesn't really work.
And I need some tests.
And you're too scared to touch the production code to make it testable because you can't remember what it does or why it works that way or which assumptions you made.
And it's just difficult to do it afterwards.
Yeah, I think there's an important point that you brought up around how
I think TDD is in some ways presented as this idea that you mentioned of like write your tests and then switch over to your production code and then only in production code get all of your tests to pass.
And I think in reality, you've got this inner loop where you're going back and forth between your test code and your production code and you're modifying them both at some point
And that is, I think, not generally the way that TDD is presented as the way that you should do it, but the reality to be, I think, productive is that you kind of do have to be going back and forth, adapting things.
I feel like when Luciano introduced it, that's the way he did it.
He goes, okay, what do you want to do?
All right, first write the test for that.
Okay, now implement the code.
And it was just one thing at a time.
And that did make more sense to me, but often I go, well, I'm not sure what I wanna do at this point.
I don't know what's possible or whatever, but that's just me.
There's, I think a third piece to this that I often do is think about the data model and the functions around that data model is almost like a separate thing.
Like I've got the functionality
but then I've got the data model and functions, and then I've got the tests.
And a lot of times I start with my data model first because to even write a useful test, I have to have the actual objects defined.
But then when I define like a function, I, in Kotlin used to do or in Scala use triple question mark as being like, all right, my test is going to blow up when I actually try to call this function.
And that gives me the name that I can reference, but then allows me to write the test against that function that is not yet implemented.
I guess in Java you could throw not implemented exception or something as well, but the triple question mark should end scholar to do and Cotton's pretty nice for doing that.
So I kind of like build out the data model and the functions, but then there's that like circular loop between all three of those different pieces to actually kind of get to where I want to go.
Right, right.
Um, is this a segue that we could go into?
I want to talk about, um, flaky tests.
Oh, flaky tests, right.
Cause that's a thing.
Okay.
Tell us about flaky tests.
Yeah.
What's, uh,
I mean everyone has flaky tests like if you're doing any kind of like especially integrations is only like would you only usually get a flaky test on an integration test or is this happen.
I mean, I think you probably, you're going to find flaky tests more frequently on, um, when you've got independent systems or messaging or integrations or like, yes, anything.
I mean, you can define flaky tests.
The easiest definition of a flaky test is you run a test and it fails and you run it straight afterwards and the same hardware, the same JVM, the same everything else and it passes.
So it's a non-pure functional test.
It's a non-deterministic result.
And so, you know, and obviously there's different variations in that it mostly passes, but every now and again it fails versus, you know, pass fails, pass fails, pass fails.
But ultimately, if you can run it under the same circumstances and sometimes it passes and sometimes it fails, it's flaky.
Yeah.
These are challenging because you then get your CI system that is flaky.
Like the flakiness then propagates to cause you a lot of challenges in keeping that main branch green or whatever.
And how do you solve this problem?
Yes, I come to that.
My main problem with flaky tests is that even if you have one flaky test in your whole test suite, you stop trusting your tests because you're like, you already know, sometimes it passes.
And so you start rerunning things, whether you rerun it like locally or whether you get your CI to rerun things.
this becomes inefficient, use more resources, ultimately more money.
And what does it mean to pass a test, your test when one of them is flaky?
What does that mean?
You can't trust it.
You've invested all this time writing tests, whether you did TDD or whatever you did, you've written tests, you have tests, but they are not doing the job they're supposed to do because you don't have the confidence that
The tests are doing what they say they're doing and you don't have the confidence that your production code is working properly because if your test fails sometimes you're like Is it the code?
Is it my CI infrastructure?
Like what what is it?
And if I can't trust my tests, why write them?
Well, so here's the thing is that I have an argument, my first argument for what to do about flaky tests is delete them.
Just like just delete it because I really do think that they cause more pain than any kind of
security you get from that test.
Because every time it passes, you're still not kind of tricked into believing that it actually works.
Right.
And so I think they're toxic.
I think that they lead to this lack of faith in your test suite and not to mention the cost, the time cost and all the rest of it.
So my first answer would be delete them.
But obviously you should try and fix them.
And there's a bunch of different ways to fix them.
So Dave Farley has a good video on this.
He talks about the five causes of intermittent tests.
And so there's things like infrastructure changes that can happen underneath you, especially if it's a CI environment.
It talks about things like versioning.
So you run it in one environment, and it's fine.
But in another environment, it's not fine.
But you haven't versioned in your code, like I expected to be using this service, this version of the service, or I expected to be using this version of Java or whatever.
So obviously from Dave's continuous delivery point of view, his whole thing is like everything should be codified.
You can't just rely on
it's probably there and it's probably what i expect so that's that's another thing and then of course there's a lot of things about asynchronousness and messaging and weights and timeouts and race conditions and that kind of thing when i went with david at lmax there was a we had a bunch of flaky tests they were mostly timeout based stuff because we were using asynchronous messaging
but it turns out that one subset of failing tests were failing for production reasons.
There was actually a race condition in the production code and the only way we found it is because we hammered it with a bunch of acceptance tests every 40 minutes and that would bring in this race condition in the subset of the tests.
The flaky test could be indicating to you that you've got a deeper problem, like a race condition that only surfaces given some particular state of the system.
When you're using concurrency with flaky tests.
So would it make sense to have a separate suite of flaky tests?
So you have your, your tests that always should work and then your flaky tests that you run when you're looking for deeper problems.
So, so one of the approaches I was talking, so one of them, right, I've got so many things to say.
Um, working at Gradle, we have a product called dev velocity, which does identify your flaky tests for you.
So when a test fails, it reruns it.
And then if it's, if it fails and passes in the same,
set, then it flags it as flaky.
So you get to see your flaky test.
So that's a great thing.
Visibility of a flaky test is a good thing.
So then I was discussing with my fellow developer advocates, like what do we do about flaky tests?
Do we just...
From a developer advocacy point of view is our message, you have to fix your flaky tests because they're noise and all the rest of it.
And we realized, of course, there is a subset of tests which will, by definition, be flaky.
Things like, well, flaky.
Not always give you the result that you expect.
So for example, one set of tests that we were running at this place were effectively smoke tests against third party libraries and third party systems and things.
And they can fall into different categories.
One is, you know, their test system is down.
So like the tests are going to fail.
Another is they've changed the version.
You kind of want your tests to fail if they change something and your tests expect one thing and get something else back.
And another is, you know, just integration is a point of potential failure.
You can't, you can't time out.
Your time out can't be infinite.
Sometimes stuff goes down.
So yes, anything which
The test is valuable, but there is a non-zero chance that the test might fail for probably infrastructural reasons.
Then you want to run those probably separately so that they're not part of your, whatever you call your, you know, your acceptance testing commit bill, whatever it is.
So then you can see, so you should have a set of tests, your unit tests and even end to end tests, which you can rely on so that the whole thing can go green.
like all the time in theory.
And then anything which is like, well, we just want to make sure it's probably not really stupid can be a little bit off to one side.
So that's one side.
It's part of the underlying challenge here that we typically have a red-green approach to test passing.
And there are cases where maybe it would be better to think about it in terms of a risk level or a
like probability level or like like hey you know we're a 98% confidence that like the system is is good and you know that.
rather than always saying you're either at 0% or 100% is kind of the red-green approach.
You're probably right because if you think about performance tests, there's usually some hard line on a performance test.
So we had performance requirements of a latency of under 10 milliseconds or whatever it was.
So you want your performance test to be under that, but it's not.
You can still argue it's red and green, but it is on a scale.
So like you want to be able to see when you're getting to like nine milliseconds, you should probably do something about that and bring it back down again.
So it's not just a case of it's fine or it's not fine.
It's like it's fine or it's not, it's kind of fine or it's really going to get to be not fine quite soon.
And I think that's, I mean, we live in a world where there's, it's not red and green.
It's not black and white, but everything's on a bit of a scale.
You know, it depends, you know?
Yeah.
We don't ship perfect software to production.
We know that our software is not perfect.
What level of imperfection are we okay with and still shipping?
Maybe a future improvement to CI systems would be able to say, how much flakiness do you want to allow to consider this green or able to be merged?
You should also be able to flag like I expect this to be flaky sometimes versus I did not expect this to be flaky.
I don't expect when I run into my services under my control for them to not speak to each other or whatever it is.
I think the binary idea goes unfortunately filters up into management as well.
You know when we had this idea of test coverage which hopefully is going away
You know, if the managers would go, well, of course we want 100% test coverage.
And then we had to go, oh, this was a bad idea.
We need to change this model.
But it was measurable.
So you can't manage what you can't measure.
That's what they say in business school.
That's what they say.
Right.
So we got to have, we got to have measures.
And if we're going to measure it, then if I'm going to get a promotion, then it's got to be a hundred percent.
This comes back down to the book writing.
It's like 80% complete for like two years, 20%, which is really hard.
The last few percent are the ones that it's hard to get out of.
So different approaches to dealing with flaky tasks.
Maybe the, the Uber point is that you
you should put some thought into how you're dealing with and how you're classifying flaky tests and there's different approaches that you can take.
You can do retries as part of that in the GRPC Kotlin project that I maintain.
We have a flaky test and I have
No idea why it's flaky.
I've not been able to figure that out.
And so there's a cool gradle module, gradle plugin that I use that just like retries the whole test suite multiple times.
And it's kind of a brute force approach, but ultimately, you know, I get a green build and that feels good.
But so retries at some granularity is probably one strategy that you can take.
But then I think you mentioned a few other possible approaches that you could take, and maybe something that the Great Old Develocity tool helps with.
So Develocity uses the retries thing from Great Old, but also from Maven to be like,
to flag it as flaky.
So at least you're going one step further than, I retried it, it went green, so it's all good.
I retried it, it went green, but hey, I don't think that's right.
So Develocity kind of flags that and it shows you like these are how many flaky tests you had in this build and you can look at it over time.
So you can see your most flaky tests, which is really helpful.
The other thing that came into a more recent version of Develocity is
the way that I've done flaky test detection in the past which is from build to build given that things haven't changed this test tends to go red green red green red green so it's probably flaky and so then you can prioritise them and then this allows you to actually do something about your flaky test because
When I go to conferences and I ask people, do you have flaky tests?
Some people say, I don't know.
And that worries me very deeply.
Because I know that when we work on very complex systems, which most of us are working on complex systems these days, our CI system is like.
I'm not going to say it's red all the time.
I'm going to say it's red sometimes, it's green sometimes, and I don't really know what is the real state of my tests in CI because, like you say, are they 90% passing or 1% passing?
Because it went red, and I'm not really sure what that means.
And with some, at least some visibility over the flaky tests, we know, well, this goes red 20% at the time because of flaky tests.
And
Maybe you can put them into a different part of the build or we talk about quarantining them.
I've worked in a place where you put an annotation on them, which says ignore until.
So you give yourself like two weeks to kind of like fix it.
And then if you haven't fixed it, it starts fading.
Oh, that's a cool approach.
Is this part of devalocity?
No, this was when I worked with Dave Farley and we had like, we used a JUnit annotation to... Oh, we priced our own annotation.
So the JUnit annotation like gives you a time box that you can ignore the flakiness for and then the tests are absolutely failing after that.
Yeah, it doesn't run the tests for a while and then when the date goes past, it starts running it again.
And then if it fails because you forgot to do anything about it, you're like, oh yeah, I really need to get on that thing.
It's a good point because since I added the retry to GRPC Kotlin, I have totally forgotten that I should at some point investigate that I have a flaky test.
Now it's just like, oh, the build is green all the time.
And so bug fixed when in reality the bug is not fixed.
And so that's one of the things we talk about from Gradle as well.
And the Gradle developers do this.
They schedule flaky test days so that instead of working on production code or whatever, today is a flaky test day.
So we're going to use devloss tea.
We're going to rank our tests by something.
It might be most flaky or it might be
um the ones that get run the most or whatever it is by some metric and then just go go right that's the one i'm yeah some information to guide you to what should be a higher priority than than other things yeah and you could see things like if it's obviously if it started becoming flaky in the last week then that allows you to drill in oh what was that commit what did we do or did we change something in ci in that time so you have more metrics to kind of dive in and try and troubleshoot the problem awesome
So what does dev velocity do?
So dev velocity is, I'm not sure we've got a succinct way of describing this at the moment.
It is a developer productivity tool.
It's on-prem, it's installed on-prem at organizations, and it allows you to do a number of different things.
One, it helps speed up your build.
So it provides things like build caching for Maven and Gradle.
So Maven doesn't generally have, there is an open source build cache, but dev velocity has a better one, I have to say that.
I don't know what the difference is, but there's a build cache, so it allows you to to cache the output of various builds, so you don't always have to rerun builds, you get faster builds.
So Devil Oste provides acceleration technologies, but it also provides analytics like flaky tests, build failures.
And what I think is really interesting about this is it provides it not just for CI, but for your local builds too.
So in one dashboard, I get to see
This test which passes all the time on ci is very flaky locally and it might be a resources thing or it might be a configuration thing or whatever or I can see if one of my tests fails in my build and I go to dev velocity and look at the history I can see oh this fails a whole bunch for like these people on my team.
And so I'm going to speak to those people about how they fixed it.
So it pulls together all these stats from your builds, like local builds and CI builds, and allows you to actually start using that information to make improvements.
So my experience of builds, be it Gradle or Maven or whatever, or Ant or whatever,
is that someone creates the build, and then we all tentatively touch it from time to time, and we try not to break it, and then we just kind of live with it and accept it as it is.
It's come out of the experience that the Gradle folks have had doing consulting for companies that run Gradle.
Here's all the things that you can do to improve your build.
This is how you can improve the performance.
This is how you can improve parallelization.
You need visibility over these sorts of things.
And it kind of says you shouldn't accept the build as a static thing.
It's kind of living code the same way our code is.
And if we could actually just look at it, inspect it, get some analytics and provide some tools for things like acceleration, then it doesn't have to be this annoying thing that everyone's scared of touching that slows us down.
This is something that's actually can help aid our productivity.
So in production systems, we have observability and ops.
And it sounds like this tool is essentially like ops and observability for builds.
Right, exactly.
And so I read something in release it, the second edition of release it.
And he says,
it says that we should be treating our development machines and our QA environment like the production environment because it's our environment for producing the code that goes into production.
And yet actually we quite often just ignore this or have like crappy QA environments or like cheap laptops for developers.
We don't clean stuff up after ourselves.
You know, we're just trying to get by.
And so
Things like dev velocity are kind of stepping the direction.
It's a step towards developer productivity and dev x and platform engineering and those kinds of things of like, let's take this area seriously.
Our development environments are important.
Our testing environments are important, staging, CI, all of those things should be monitored the same way that we would with a production environment.
And we should be investing time and improving those things rather than just going, well, you know, the build takes 10 minutes and it is what it is.
Right.
Well, but those things are not what the customer's paying for.
The customer's paying for that code that we're working on.
So those things are secondary.
But that's why DevLosty is kind of interested.
Developers are free.
Yeah.
The dev velocity is interesting because then you've got statistics.
So you can say, let's say you've got a 10 minute build time for everyone on the team when they build locally.
Now you have stats for how long the build takes, how frequently your developers run that build.
And then all you have to do is times that by your average developer salary and figure out, oh my God, we are losing so much money just because we haven't invested a couple of days in figuring out how to reduce our build time.
But this can backfire and make developers unhappy, because when are they going to go out and sword fight in the hallway?
I was going to do a video about that.
I was going to do a video about when developers don't want to improve their productivity.
Actually, that could be very funny.
I've worked in environments like that.
I need these long builds, so I have time to look at Twitter.
Right, exactly.
I need to, or I need to, I don't know, go out for a smoke, whatever it is that people do.
We used to, actually, so here's a story that's going to go in that video, I hope.
When I used to work at a big investment bank in London, we had three hour release times.
And so, and we would have to kick off the release end of day, 6pm.
This is clearly when I didn't have children or a family or a life, actually.
So we would do these releases.
This is released to QA, by the way, and we would have to do this every week for a three month period while we're going through the testing phase.
So we would kick off the release process, which starts off with database migration or whatever it is.
We'd go downstairs to the pub.
We have enough time for a drink each.
We come upstairs, we kick off the next process, go downstairs for another drink, come back upstairs.
And because I'm not going to sit in an empty office, it was me and my colleague, I'm not going to sit there in an empty office for three hours staring at a progress bar.
And so we did talk about improving that build process.
And there's a lot of pushback, the normal stuff of, oh, you can't automate this because what happens when it goes wrong?
It's like, it's literally 12 pages of scripted things to do.
That seems like something we should be able to automate, right?
And then we could actually kick that off and go to the pub for three drinks in a while and then come back.
I have to go back and forth.
Yeah.
So yeah, so there's times when we don't necessarily want to improve our productivity.
Similar sorts of organizations where if as developers we don't really feel valued and we do feel like we're measured by the amount of time we sit on the chair and in front of the computer,
why would I want to be more effective and more productive during that time when you're literally just measuring bum time on the seat?
I can kind of, I would quite happily manually run this script and manually copy this from here to here because I'm filling my time with something useful instead of optimizing it so I can do something else.
Why?
It feels like there are fairly easy, straightforward ways to improve developer productivity.
And yet it seems like generally organizations do not invest in improving productivity.
Just as like a silly example, I think a lot of especially enterprise developers work on really crappy machines.
And organizations could spend a few thousand dollars per developer, get them good machines that actually aren't super slow and always running out of memory and whatever, but they don't.
Because of accounting.
Yeah.
But why has it been so hard to convince organizations that developer productivity is worth it?
It's worth investing in.
I don't really know the, oh, I don't know the answer to that question.
I do know the laptops is a good one.
I worked at another investment bank in London where I went there.
It was when I was working at ThoughtWorks and I went there with my ThoughtWorks Mac and then the bank gave me this thing.
They're like, here's your laptop from 1982 that you can use to build the software with.
Don't hurt your back picking it up.
I'm like, this is not portable.
And what do you expect me to do with this thing?
It takes, and I could measure it compared to the Mac.
It was a MacBook Air.
It wasn't like some super powered Mac, but the MacBook Air would run things four times faster at least than this brick, right?
And so here's an interesting thing.
And this is something I've been finding with dev velocity is that
One of the problems is that I don't want to blame developers, but as developers, we've been told that we don't have any power over this and we are kind of at the mercy of accounting.
But once you start collecting some statistics, like with DevLosty, we saw that we collected statistics of things like build times.
So we've done this study where build times for one organization, I can't remember what the overall build time was, but let's say it's 20 minutes because it's not unusual.
And you build it even on an M1 Mac and then you build it on an M2 Mac and it takes like half the time.
And then they did the calculation of time versus developer salary.
And like how long will this pay itself off?
Like if we buy everyone an M2 Mac, like how long will it take to pay that off?
Like.
six weeks or less, you know, some crazy, because we're measuring for all the wrong things.
And if you can measure things, if you can find things to measure, which impact our productivity, which is why build times is kind of interesting one, because you can measure that.
And then start putting financial numbers on this.
Then someone else goes, Oh, yeah, that's, that's a lot of money.
We should probably do something about that.
So speak in the past.
Developers have been like, my build is slow.
And accounting goes, okay,
Right, we don't know what to do with that.
Yeah, so you're saying, okay, acknowledge that the accountants are running things and just speak in their language, which is, I'd say, step one, but step two would be maybe we should put decision-making.
Some decisions should be made not just by the accountants.
Right.
I've been speaking to some potential customers of dev velocity and some of the effective ways of making changes for developer productivity is a sandwich approach.
So someone at like maybe sea level or certainly some sort of like senior engineering level decides developer productivity is important.
Our developers are expensive.
They're one of our most expensive resources.
Let's get the most out of them.
Not like squeezing lines of code out of them, but like let's
enable them to be good at their job this is this is an important thing for this company so someone at the top level says that and the developers go great this is definitely something that i want i don't want to be wasting time waiting for staff or struggling with a terrible laptop or
waiting for CIQ times, which are like 90 minutes till my build gets in there.
So if you have the top level and the developer level, you can kind of effectively squeeze in towards middle management and push in both directions to affect the kind of change.
Because often what you find at the mid-level, there's a lot of competing different objectives for competing different managers, and a lot of them are just obviously trying to push out features and not thinking about like developer productivity.
So if you can target it from the top level and the bottom at the same time, then you can really start to affect change.
Well, I'm seeing the future as well we have
We have human limitations in all of these things.
You know, the accountants see it from their viewpoint, the CEOs see it from whatever they're, and I'm thinking, we are gonna eventually replace the C-suite with AI, and they can see, that'll see everything.
And it won't be these little siloed bits of knowledge that are limited by what that person has been exposed to.
And companies are just gonna run so much better.
But yes, I would love that.
But I also feel like an AI is exactly the same as a human being.
It learns based on the weights you give to certain objectives.
So the AI is still going to optimize for something.
Or what are they going to optimize for?
It is.
But I'll say the difference is the human being can hold whatever.
You know, five easily ignore information.
Well, easily ignore information and can only hold whatever five to seven things in its mind at one time.
So their equation is extremely limited.
Whereas the AI can hold all of these factors.
And that's the difference.
It's true and for the AI, let's say, let's like, yeah, so the AI, if you had multiple AIs at the sea level, instead of them having to have a whole bunch of meetings all the time to sync, they're just kind of like sending messages to each other.
Yeah.
And I don't know that you would have all these little signs.
No, no, no, exactly.
It would just be going, oh, look, the developers are having trouble here.
We need to fix that so the whole company moves better.
And it isn't a matter of, well, I don't understand what long build times are.
Right.
the AI would know that and it would go, how can we improve all of these things at once?
So one idea to run by you, we talk about developer productivity and we had a podcast a while back around the developer happiness.
One piece of developer productivity is, do developers get fulfillment and enjoyment out of their flow and being able to be in flow state?
But then there is the accounting side, at least pre-AI.
And the accounting side, I think when we say developer productivity, accounting doesn't care.
But if we said developer efficiency, like if we put it into the efficiency terminology, then there is something that maybe we could, and maybe with the velocity, we could actually have reports that illustrate like, oh, here's ways that we could actually make developers more efficient.
Yeah, I don't know.
It seems like maybe by classifying everything as developer productivity, we're not actually accomplishing the goals that the developer wants and we're not accomplishing the goals that the business wants.
It's like we've kind of framed the whole thing wrong.
Yes, I liked what you said about developer happiness because to me the reason why I'm really excited about developer productivity is not because I want developers to ship more lines of code or even put more features in the hands of users but because like you said like when I'm in the flow state I'm happy, when I'm like when I don't have to fight with my laptop or I don't have to wait for a
million years for CI to give me the answer that I want, then I'm happy and I'm creative.
And so for me, developer productivity equals happiness.
But then we all saw the McKinsey report, which was like developer productivity is who to fire because they're not productive, which is not what we're talking about here.
Yeah, exactly.
Yeah, so there's all these different kinds of lenses to look at it through.
We've kind of lumped them all under developer productivity, and maybe that's part of the challenge is that we should break it down into the more important things, which for us developers is, yes, I want to be in the flow state more.
That is a fascinating thing.
I mean, the McKinsey report that you just brought up is like, oh yeah, you can take this information and you filter it through your lens
which is, oh, well, if they're not productive, get rid of them and get somebody in who that's the solution.
Whereas the other solution would be, well, in all the layoffs that we're seeing now, I look at that and I'm going, so you spent all this energy finding these people and then as soon as a number flips in the wrong direction or somebody incentivizes you in a different way, you go, oh yeah, we'll just throw them away.
You're going, that's so much.
The question wasn't how can we make them more productive as well.
How do we?
Yeah, well, I mean, we're a publicly held corporation.
Our obligation is to maximize shareholder profits.
So that's what we do.
And you're looking at it, you're going, yeah, but in the big picture, that's not maximizing shareholder profits.
So dumb isn't it you just know like you say all that talent goes away all the goodwill from the developer community kind of you lose a lot of that.
And the productivity goes down all the people who are left behind demoralized doing twice as much work and they're scared they're scared they're going to be let go as well.
So in a not very long term, and within 12 months, it's already a losing policy.
Oh, right.
Well, and that's the thing, the inability of the sea level to look at the big picture, they are limited in their view.
And again, I'm
you know, I'm advocating for smarter systems and that would be, let's let the machines do it once they get better enough.
I think we will.
I think, I mean, in my opinion, the VCs are probably gonna start doing this.
They're gonna go, well, if we can increase our hit rate, let's put the machines, you know, managing things, because then we're gonna make more money.
So Trisha, what do you do when potential customer of Develocity comes to you and says, can you give me a report of my lowest performing developers?
I had a conversation with the CEO about this this week because we're talking about
how to talk about dev velocity in the context of space metrics and space and Dora and those sorts of things.
And the CEO was quite clear.
He was like, we will not use dev velocity metrics for individual performance.
We fundamentally don't agree with that.
The aim is to empower developers so that we are in a state of flow and happy and not facing friction and toil and slow build times.
Yeah, but the answer to your question is it doesn't matter what you do.
You're already screwed when they ask you that question.
Yeah, because what they're looking for is how do they cut costs, not how do they make their developers more efficient?
Sure.
Right.
And we quite often talk about the idea of being to increase throughput.
So you're not
not going to cut costs, you're going to try and increase throughput.
So don't fire your developers.
Why don't we just get more out of them?
And that will make, if you can produce more, whatever that is, more features, or, you know, I don't even know how it works in various ways.
If the code that the developers are producing is going to
increase revenue then we should be focusing on that not on decreasing headcount because as you've already said like decreasing headcount doesn't make any sense because you're going to decrease costs sure but you're also decreasing throughput and then you're also decreasing your ability to improve that in the future because now everyone hates you.
Switching gears real quick.
As we were talking about observability for build systems, I wanted to ask you if you've seen this concept that I saw somebody talking about on Twitter a while back, which was the concept was, or the question was, why don't we write tests for a build system?
We write tests for our software.
to verify that our production system is going to work.
Why don't we write tests to validate that our build system is doing what we expect it to, and doing it in the amount of time that we expect it to, and so forth.
Have you seen this idea explored, or is anybody doing this?
I think this is a great idea.
And it's ringing bells for me because one of the reasons we switched to Gradle a long time ago in one of the places I worked is because you could separate out the build logic into modules that you could test.
We tended not to.
We were using AMP before as well.
So you could do it with AMP because you can write little Java code and test that.
I mean, it's kind of difficult because sometimes what you want to test is it moves a file from here to here.
And like, that's the sort of difficult thing to unit test, right?
Yeah.
So, yeah, I mean, I would love to test a build system and check that it's doing what I think it's doing.
And I think the first thing that
that one of the things I like about Devalosti, not to pound it too hard, because actually, I'm still fairly new to to Gradle.
And I'm still kind of, in many ways, I'm still trying to get them to sell to me.
Like, what is this product and what's it for?
Because like, I'm not the kind of advocate who's just going to go out and sell a product.
I really have to believe in it.
But one of the things I like about Devalosti, and I was using it today in a video, is
well it's actually not even dev velocity it's the free build scans so that's nice it's free so anyone can use it.
With a build scan you can see like visually the parallelism of your build so you can see like it's run five different threads and this is where the tasks were run and I really like that because it's not quite the same as an automated test but it's at least some kind of
feedback into what is happening in the build.
And so I use these build scans to be like, I want to tune the build now I want to, with this build I was trying to add parallelization, add parallelism, add the build cache.
And then I wanted to also parallelize the
individual tests, which is a separate thing too.
And so I can use the build scan and look at them side by side, and obviously I can look at the overall time, but I can see, are these things running parallel?
Which things came from the cache versus which compilations needed to happen again?
And you can even do side by side comparison in terms of like, things like, which dependencies did this build use versus this build?
So, I mean, that's kind of the first step.
So you were using a tool to give you that information, and maybe some future could allow us to actually write a test to describe, yes, I want my test to run in parallel.
This test should validate that my tests were run in parallel.
Right.
So we have some validation scripts at Gradle.
When we go onto a customer site and set them up with dev velocity for the first time, there's two things we do.
One, we set up dev velocity.
But the other thing we do is we start optimizing their build for them so they start seeing the improvements of the acceleration part of dev velocity.
And that is scriptable.
So it does things like checks out an individual commit, runs it, checks it out to a different location, runs the build, and then sees if there's any differences and that kind of thing.
And then you get like pass or fail.
These things, the cache worked as expected because when I reran it, I got it from cache or the cache didn't work as expected because I reran it and I had to rerun everything.
And so you can automate some of those kinds of things and
and use those things to also check for regressions so let's say someone goes in in three months time and decides that they're going to one of the things that stops the cash from working is things like generated timestamps that kind of thing because obviously that's like.
that a newly generated random number or timestamp, it can't... Somebody changes the build.
They've added something that has made part of the build uncashable and build times go up, but you have no idea why.
Exactly.
So if you've got validation scripts and you're running them regularly, it can say this thing is no longer cached.
So we've only got like four of these scripts.
So it's a start.
It's a very small start towards it.
Yeah, I don't see why not.
I don't see why we shouldn't be testing our build, certainly for performance, but also like, is it really doing what we think it's doing?
Is it caching stuff when I expect it to?
Producing the artifacts in the way that we expect them to be.
Exactly.
Is it like doing the right hash codes?
I don't even know.
Whatever it is that builds do.
We should be able to
check that the build is doing what we think it's doing.
Yeah, yeah, that'd be fun to explore.
Cool.
Well, that's fun talking about builds and productivity and all that.
Yeah, I think the thing that I'm most cognizant of from this conversation is the idea that all of that stuff
I mean, it's hard enough for somebody in management to understand what it is that we're doing in the core of it.
And then the peripheral ideas of, well, how long does, you know, how long do builds take?
What are developer productivity things?
That's just too much.
And so that's why I think we run into these problems where we're looking at it and we're going, those are first class elements.
And from somebody who you're trying to explain what it is that we do, that's just like extra too much.
What do we get money from?
We get money from the code that we ship.
Those are other things.
Those are just expenses.
Yeah, or something.
Those are peripheral.
And how do we make that?
Well, and even as developers for the longest time, we weren't thinking of that.
Oh, I'll just rerun the compiler.
And there's still places out there that are just not using automated builds.
I think that's part of the problem.
I think as developers, we've got really used to putting up with some crap, like long build times or
flaky tests or a laptop that doesn't do what we want or we just we go from environment to environment and there's always like in whichever office we work in there's something which is just.
This is something we just have to put up with as developers one of the things we can do to help our productivity is.
start raising those kinds of start looking at things which really slow us down.
You know what it does drive me crazy that the build is six minutes, which to me, a six minute build time seems reasonable.
But six minutes is not enough time to do anything else.
But it's too long to sit there staring at the build.
I can start talking about these and these taking some metrics, these are the things which slow me down.
Here's a metric that I get a lot with intelligent idea.
If you change branches, when it re-indexes everything, it takes x amount of time, right?
And these are things where we can be like, you know what?
I do this three times a day and it takes this amount of time.
And what can we do to improve these things so that I don't have to get jolted out of my flow and I can still be doing that lovely thing of, oh, I'm just going to fix this problem.
That lovely thing.
Oh, yeah, the whole flow thing, which is ultimately psychology.
And that's even further outside of the realm of what so many people are used to thinking in terms of.
But it's essential.
I tried to do a video about how IntelliJ IDEA helps us to stay in the flow.
I've been trying to do this video for like six years, by the way.
And it's so hard because I'm like, but it's just flow, like it just is.
And you can't fake flow in a screencast because you can't explain it while you're doing it.
It might be one of the things that I've run into recently is trying to discover, well, trying to prove why inheritance and exceptions
are problems, why they don't scale, why they don't compose.
And I've sort of come to the tentative conclusion that, oh, I'm not going to be able to prove that.
Right.
And then so I need to take a different tack because I can't give a solid, you know, because people are using these things and they go, they seem to work.
Yeah.
And your video may be encountering the same problem.
You're trying to solve it in a way that you can't.
I think you're probably right.
How do you know you're in the flow state?
When you're in the flow state, you feel it.
Yeah, it's like pornography.
I know it when I see it.
I know it when I'm in it.
The inheritance thing is interesting because I was poking through a code base the other day.
I'm trying to parallelize the tests, actually, because the tests take the longest time.
And the tests all inherit from an abstract test, which inherits from an abstract test.
And I was like, no, why are you doing this?
And this is an experienced programmer who's done this, like, how do I say, don't do that, apart from just saying, don't do that.
Trust me, it's just not working.
Yeah, we were talking about the most important statement in the Gang of Four design patterns book, which is prefer composition to inheritance, and it should probably be
Don't do inheritance.
But yeah.
And it's like, it's the same thing.
You can't really, especially because we've had all this background and it worked in small talk and all this kind of stuff.
It's like, why?
The problem is, if somebody asks, why shouldn't I do inheritance?
Then it's like, oh, I can't explain the why.
Just don't.
It gets nasty when it gets nasty, right?
And then the inheritance thing, when you're talking about, you know, the duck thing that they do when hit first Java, inheritance is kind of fine.
But in these tests, these tests, for example, there's like 200 tests, they're integration tests.
It has this huge set up and teardown in one abstract method, a different set up and teardown in the other abstract method.
And this is only really nasty when you're talking about hundreds of tests.
But you have to see that thing scale.
It doesn't scale.
See, that's the problem with all of these things, with exceptions, with inheritance and everything.
They seem great at first.
And in the small, it looks simple.
It looks direct.
But then you try and scale it.
And you go, oh, for some reason, this isn't really working, like in the small.
We talked about this, well, back this idea that decision making through, through like quorum or something works great until you get to like five or seven people.
And then you can no longer use that model for decision making.
Yes.
It's a lot of parallels to like.
I think in terms of, I think we were talking about in terms of like language design and stuff, you have like three people and everything works really great.
You go, well, I'll just make that bigger.
And it's like, nope, doesn't work because of the psychological factors.
Right yeah and we don't argue like trying to make decisions at home so I have a husband and two children honestly more than one person involved in the decision making is too difficult especially when you're talking about like well he's working now and then but I'll be working later and then he's got a meeting and then the kids have got a thing and then you don't speak for like three days and it would have been easy just to make the decision there and then and even if it's wrong it's just better.
Yeah.
Well, that's something I took a workshop in holocracy from the guy who created this idea.
And the thing that mostly I got out of it was we front load our decisions with... When we make a decision, we go through all of this work to make sure it's the right decision.
because if we have to revisit it, we assume we'll have to go through all that work again.
And so this approach is you make the quickest decision that you can that doesn't have any obvious, you know, disproval.
And then if it doesn't work, then you revisit it and you again make a quick decision rather than trying to presciently know
everything about it when you make the decision, which makes it so heavyweight that you never want to revisit it.
I think you've just changed my life.
I think that would make my life a lot easier.
Yes.
As part of that, there's also the concept of like being able to perform kind of experiments that then you can validate to make the right decision or something.
Yes, little experiments.
Yeah, exactly.
It's the same, I think it's the same sort of mindset.
Yeah.
Anything else, Tricia?
No, I should probably go and check on my children.
I have left them being babysat by Minecraft.
I do that as well.
It's a good babysitter.
Well, thank you so much for joining us.
That was super fun.
Thank you for having me.
This is really great.
All right.
Bye.
